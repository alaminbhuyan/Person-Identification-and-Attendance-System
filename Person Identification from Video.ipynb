{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30430,
     "status": "ok",
     "timestamp": 1649291863514,
     "user": {
      "displayName": "Al Amin",
      "userId": "17120997331533597303"
     },
     "user_tz": -360
    },
    "id": "xaVQNaANcw1V",
    "outputId": "c388f554-2991-4ab5-ead3-dab9cdc8dc0e"
   },
   "outputs": [],
   "source": [
    "# Change run time type and select GPU mode\n",
    "\n",
    "!pip install face-recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3430,
     "status": "ok",
     "timestamp": 1649291896035,
     "user": {
      "displayName": "Al Amin",
      "userId": "17120997331533597303"
     },
     "user_tz": -360
    },
    "id": "tQ9ob8S9dQJ_"
   },
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from google.colab.patches import cv2_imshow\n",
    "import openpyxl\n",
    "import datetime\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1649292674112,
     "user": {
      "displayName": "Al Amin",
      "userId": "17120997331533597303"
     },
     "user_tz": -360
    },
    "id": "E2QZmqpKgLqV"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript, Image\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode, b64encode\n",
    "import PIL\n",
    "import io\n",
    "import html\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1649291918214,
     "user": {
      "displayName": "Al Amin",
      "userId": "17120997331533597303"
     },
     "user_tz": -360
    },
    "id": "j_X5i5bBdRD_",
    "outputId": "d0eef21c-0514-4dfa-cac8-65272ef5626d"
   },
   "outputs": [],
   "source": [
    "print(face_recognition.__version__)\n",
    "print(cv2.__version__)\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0B4lfVf0gEbO"
   },
   "source": [
    "# **Face Indetification from Video or Webcam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 676,
     "status": "ok",
     "timestamp": 1649294707769,
     "user": {
      "displayName": "Al Amin",
      "userId": "17120997331533597303"
     },
     "user_tz": -360
    },
    "id": "3uClU2xPgC_1"
   },
   "outputs": [],
   "source": [
    "path_video = \"Images/train_img\"\n",
    "\n",
    "images_video = []\n",
    "\n",
    "classnames_video = []\n",
    "\n",
    "myList_video = os.listdir(path=path_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 620,
     "status": "ok",
     "timestamp": 1649294711338,
     "user": {
      "displayName": "Al Amin",
      "userId": "17120997331533597303"
     },
     "user_tz": -360
    },
    "id": "MSmB09HTgsDX",
    "outputId": "0ca2d73b-cf64-4f70-d00c-5fede3b15b67"
   },
   "outputs": [],
   "source": [
    "print(myList_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 559,
     "status": "ok",
     "timestamp": 1649294717337,
     "user": {
      "displayName": "Al Amin",
      "userId": "17120997331533597303"
     },
     "user_tz": -360
    },
    "id": "YcMPNdcgguZg",
    "outputId": "e7a0dfff-6a92-4f8f-b21b-7a6318da4cb2"
   },
   "outputs": [],
   "source": [
    "for img in myList_video:\n",
    "    currentImg_video = cv2.imread(filename=f\"{path_video}/{img}\")\n",
    "    # if currentImg.shape[0] > 600 and currentImg.shape[1] > 600:\n",
    "    #     currentImg = cv2.resize(src=currentImg, dsize=(600, 600))\n",
    "\n",
    "    images_video.append(currentImg_video)\n",
    "    split_name = img.split('-')[0]\n",
    "    classnames_video.append(split_name)\n",
    "    # only_name = [x.isdigit() for x in img].index(True)\n",
    "    # classnames_video.append(img[:only_name])\n",
    "\n",
    "    # classnames.append(os.path.splitext(only_name)[0])\n",
    "\n",
    "print(\"The className: \", classnames_video)\n",
    "print(\"Total Images: \", len(classnames_video))\n",
    "print(\"Images list: \",  len(images_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 986,
     "status": "ok",
     "timestamp": 1649294742322,
     "user": {
      "displayName": "Al Amin",
      "userId": "17120997331533597303"
     },
     "user_tz": -360
    },
    "id": "BlvRXp3DhVeN"
   },
   "outputs": [],
   "source": [
    "def findEncodings_video(images):\n",
    "    encodeList_video = []\n",
    "\n",
    "    for img in images_video:\n",
    "        img = cv2.cvtColor(src=img, code=cv2.COLOR_BGR2RGB)\n",
    "        encode = face_recognition.face_encodings(face_image=img)[0]\n",
    "        encodeList_video.append(encode)\n",
    "    return encodeList_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4923,
     "status": "ok",
     "timestamp": 1649294750919,
     "user": {
      "displayName": "Al Amin",
      "userId": "17120997331533597303"
     },
     "user_tz": -360
    },
    "id": "SZje5Br1h1MI"
   },
   "outputs": [],
   "source": [
    "encodeListKnown_video = findEncodings_video(images=images_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15IY97Tph-1T"
   },
   "outputs": [],
   "source": [
    "# print(encodeListKnown_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 972,
     "status": "ok",
     "timestamp": 1649293274283,
     "user": {
      "displayName": "Al Amin",
      "userId": "17120997331533597303"
     },
     "user_tz": -360
    },
    "id": "p5XzeaegiYq3"
   },
   "outputs": [],
   "source": [
    "# function to convert the JavaScript object into an OpenCV image\n",
    "def js_to_image(js_reply):\n",
    "  \"\"\"\n",
    "  Params:\n",
    "          js_reply: JavaScript object containing image from webcam\n",
    "  Returns:\n",
    "          img: OpenCV BGR image\n",
    "  \"\"\"\n",
    "  # decode base64 image\n",
    "  image_bytes = b64decode(js_reply.split(',')[1])\n",
    "  # convert bytes to numpy array\n",
    "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
    "  # decode numpy array into OpenCV BGR image\n",
    "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
    "\n",
    "  return img\n",
    "\n",
    "\n",
    "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
    "def bbox_to_bytes(bbox_array):\n",
    "  \"\"\"\n",
    "  Params:\n",
    "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
    "  Returns:\n",
    "        bytes: Base64 image byte string\n",
    "  \"\"\"\n",
    "  # convert array into PIL image\n",
    "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
    "  iobuf = io.BytesIO()\n",
    "  # format bbox into png for return\n",
    "  bbox_PIL.save(iobuf, format='png')\n",
    "  # format return string\n",
    "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
    "\n",
    "  return bbox_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 961,
     "status": "ok",
     "timestamp": 1649293344618,
     "user": {
      "displayName": "Al Amin",
      "userId": "17120997331533597303"
     },
     "user_tz": -360
    },
    "id": "Akh59FDjijR_"
   },
   "outputs": [],
   "source": [
    "# JavaScript to properly create our live video stream using our webcam as input\n",
    "def video_stream():\n",
    "  js = Javascript('''\n",
    "    var video;\n",
    "    var div = null;\n",
    "    var stream;\n",
    "    var captureCanvas;\n",
    "    var imgElement;\n",
    "    var labelElement;\n",
    "    \n",
    "    var pendingResolve = null;\n",
    "    var shutdown = false;\n",
    "    \n",
    "    function removeDom() {\n",
    "       stream.getVideoTracks()[0].stop();\n",
    "       video.remove();\n",
    "       div.remove();\n",
    "       video = null;\n",
    "       div = null;\n",
    "       stream = null;\n",
    "       imgElement = null;\n",
    "       captureCanvas = null;\n",
    "       labelElement = null;\n",
    "    }\n",
    "    \n",
    "    function onAnimationFrame() {\n",
    "      if (!shutdown) {\n",
    "        window.requestAnimationFrame(onAnimationFrame);\n",
    "      }\n",
    "      if (pendingResolve) {\n",
    "        var result = \"\";\n",
    "        if (!shutdown) {\n",
    "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
    "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
    "        }\n",
    "        var lp = pendingResolve;\n",
    "        pendingResolve = null;\n",
    "        lp(result);\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    async function createDom() {\n",
    "      if (div !== null) {\n",
    "        return stream;\n",
    "      }\n",
    "\n",
    "      div = document.createElement('div');\n",
    "      div.style.border = '2px solid black';\n",
    "      div.style.padding = '3px';\n",
    "      div.style.width = '100%';\n",
    "      div.style.maxWidth = '600px';\n",
    "      document.body.appendChild(div);\n",
    "      \n",
    "      const modelOut = document.createElement('div');\n",
    "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
    "      labelElement = document.createElement('span');\n",
    "      labelElement.innerText = 'No data';\n",
    "      labelElement.style.fontWeight = 'bold';\n",
    "      modelOut.appendChild(labelElement);\n",
    "      div.appendChild(modelOut);\n",
    "           \n",
    "      video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      video.width = div.clientWidth - 6;\n",
    "      video.setAttribute('playsinline', '');\n",
    "      video.onclick = () => { shutdown = true; };\n",
    "      stream = await navigator.mediaDevices.getUserMedia(\n",
    "          {video: { facingMode: \"environment\"}});\n",
    "      div.appendChild(video);\n",
    "\n",
    "      imgElement = document.createElement('img');\n",
    "      imgElement.style.position = 'absolute';\n",
    "      imgElement.style.zIndex = 1;\n",
    "      imgElement.onclick = () => { shutdown = true; };\n",
    "      div.appendChild(imgElement);\n",
    "      \n",
    "      const instruction = document.createElement('div');\n",
    "      instruction.innerHTML = \n",
    "          '<span style=\"color: red; font-weight: bold;\">' +\n",
    "          'When finished, click here or on the video to stop this demo</span>';\n",
    "      div.appendChild(instruction);\n",
    "      instruction.onclick = () => { shutdown = true; };\n",
    "      \n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      captureCanvas = document.createElement('canvas');\n",
    "      captureCanvas.width = 640; //video.videoWidth;\n",
    "      captureCanvas.height = 480; //video.videoHeight;\n",
    "      window.requestAnimationFrame(onAnimationFrame);\n",
    "      \n",
    "      return stream;\n",
    "    }\n",
    "    async function stream_frame(label, imgData) {\n",
    "      if (shutdown) {\n",
    "        removeDom();\n",
    "        shutdown = false;\n",
    "        return '';\n",
    "      }\n",
    "\n",
    "      var preCreate = Date.now();\n",
    "      stream = await createDom();\n",
    "      \n",
    "      var preShow = Date.now();\n",
    "      if (label != \"\") {\n",
    "        labelElement.innerHTML = label;\n",
    "      }\n",
    "            \n",
    "      if (imgData != \"\") {\n",
    "        var videoRect = video.getClientRects()[0];\n",
    "        imgElement.style.top = videoRect.top + \"px\";\n",
    "        imgElement.style.left = videoRect.left + \"px\";\n",
    "        imgElement.style.width = videoRect.width + \"px\";\n",
    "        imgElement.style.height = videoRect.height + \"px\";\n",
    "        imgElement.src = imgData;\n",
    "      }\n",
    "      \n",
    "      var preCapture = Date.now();\n",
    "      var result = await new Promise(function(resolve, reject) {\n",
    "        pendingResolve = resolve;\n",
    "      });\n",
    "      shutdown = false;\n",
    "      \n",
    "      return {'create': preShow - preCreate, \n",
    "              'show': preCapture - preShow, \n",
    "              'capture': Date.now() - preCapture,\n",
    "              'img': result};\n",
    "    }\n",
    "    ''')\n",
    "\n",
    "  display(js)\n",
    "  \n",
    "def video_frame(label, bbox):\n",
    "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 21339,
     "status": "ok",
     "timestamp": 1649296359206,
     "user": {
      "displayName": "Al Amin",
      "userId": "17120997331533597303"
     },
     "user_tz": -360
    },
    "id": "udfYVW2_jHmb",
    "outputId": "f9dd12ae-7073-4b99-8615-61047271cf92"
   },
   "outputs": [],
   "source": [
    "xlsx_path = \"demo.xlsx\"\n",
    "\n",
    "wb = openpyxl.load_workbook(xlsx_path)\n",
    "\n",
    "ws = wb.active\n",
    "\n",
    "stu_info = {\n",
    "    'rita15'   : '2191081015',\n",
    "    'alamin30' : '2191081030',\n",
    "    'taneem31' : '2191081031'\n",
    "}\n",
    "\n",
    "stu_lis = []\n",
    "stu_lis2 = []\n",
    "\n",
    "\n",
    "def takeAttendance(name):\n",
    "    name_only = [i.isdigit() for i in name].index(True) # index number '7'\n",
    "    myName = name[:name_only] # alamin\n",
    "    cdate, ctime = str(datetime.datetime.now()).split()\n",
    "    if len(stu_lis) >= 0:\n",
    "        stuID = stu_info[name] # name = 'alamin30'\n",
    "        if [myName, stuID, cdate] not in stu_lis:\n",
    "            stu_lis.append([myName, stuID, cdate])\n",
    "            stu_lis2.append([myName, stuID, cdate, ctime])\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "# start streaming video from webcam\n",
    "video_stream()\n",
    "# label for video\n",
    "label_html = 'Capturing your face...'\n",
    "# initialze bounding box to empty\n",
    "bbox = ''\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    js_reply = video_frame(label_html, bbox)\n",
    "    if not js_reply:\n",
    "        break\n",
    "\n",
    "    # convert JS response to OpenCV Image\n",
    "\n",
    "    # img = js_to_image(js_reply[\"img\"])\n",
    "\n",
    "    unknown_face = js_to_image(js_reply[\"img\"])\n",
    "    # print(\"Unknown Face shape: \",unknown_face.shape)\n",
    "    # print(\"Unknown Face ndim: \",unknown_face.ndim)\n",
    "\n",
    "    # create transparent overlay for bounding box\n",
    "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
    "    # print(\"bbox_array shape: \",bbox_array.shape)\n",
    "    # print(\"bbox_array ndim: \",bbox_array.ndim)\n",
    "    # print(bbox_array)\n",
    "\n",
    "    # grayscale image for face detection\n",
    "    # gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    unknown_face_RGB = cv2.cvtColor(src=unknown_face, code=cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    facesCurrentFrame = face_recognition.face_locations(img=unknown_face_RGB)\n",
    "    # print(\"facesCurrentFrame\",facesCurrentFrame)\n",
    "    # print(\"*\"*60)\n",
    "\n",
    "    encodesCurrentFrame = face_recognition.face_encodings(face_image=unknown_face_RGB, known_face_locations=facesCurrentFrame)\n",
    "    # print(\"encodesCurrentFrame\",encodesCurrentFrame)\n",
    "    # print(\"*\"*60)\n",
    "\n",
    "\n",
    "    name = \"\"\n",
    "\n",
    "    for encodedFace, faceLocation in zip(encodesCurrentFrame, facesCurrentFrame):\n",
    "        matches = face_recognition.compare_faces(known_face_encodings=encodeListKnown, face_encoding_to_check=encodedFace)\n",
    "\n",
    "        # print(\"faceLocation: \",faceLocation)\n",
    "        # print(\"*\"*60)\n",
    "\n",
    "        # print(\"matches: \",matches)\n",
    "        # print(\"*\"*60)\n",
    "        faceDistance = face_recognition.face_distance(face_encodings=encodeListKnown, face_to_compare=encodedFace)\n",
    "        \n",
    "        matchIndex = np.argmin(faceDistance)\n",
    "\n",
    "        if matches[matchIndex]:\n",
    "            name = classnames_video[matchIndex]\n",
    "\n",
    "            extract_name = [m.isdigit() for m in name].index(True)\n",
    "            only_name = name[:extract_name].capitalize()\n",
    "\n",
    "            print(\"Extract Name: \", only_name)\n",
    "            # print(\"*\"*60)\n",
    "            print(\"Student Name: \", name)\n",
    "            # print(\"*\"*60)\n",
    "\n",
    "            y1, x2, y2, x1 = faceLocation # faceLoc:  (63, 162, 138, 88)\n",
    "\n",
    "            # bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "            bbox_array = cv2.rectangle(img=bbox_array,pt1=(x1, y1), pt2=(x2, y2), color=(255,0,255), thickness=2)\n",
    "\n",
    "            cv2.rectangle(img=unknown_face,pt1=(x1, y1), pt2=(x2, y2), color=(255,0,255), thickness=2)\n",
    "            cv2.rectangle(img=unknown_face,pt1=(x1, y2-35), pt2=(x2, y2), color=(255,0,255), thickness=cv2.FILLED)\n",
    "            cv2.putText(img=unknown_face, text=only_name, org=(x1+6, y2-6), fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                        fontScale=1, color=(255,255,255), thickness=2)\n",
    "            \n",
    "            takeAttendance(name=name)\n",
    "    \n",
    "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
    "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
    "    # print(bbox_bytes)\n",
    "    bbox = bbox_bytes\n",
    "\n",
    "    cv2_imshow(unknown_face)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "print()\n",
    "print(stu_lis)\n",
    "print()\n",
    "print(stu_lis2)\n",
    "\n",
    "for row in stu_lis2:\n",
    "    ws.append(row)\n",
    "    \n",
    "wb.save(filename=xlsx_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNSD76YcDcnzJ0E9bbTPJfY",
   "collapsed_sections": [],
   "mount_file_id": "11mmR-q0tZn8w5OSk-W1LqlolLGersmLz",
   "name": "Person Identification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
